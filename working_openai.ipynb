{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 4966565,
     "sourceType": "datasetVersion",
     "datasetId": 2880535
    },
    {
     "sourceId": 7239614,
     "sourceType": "datasetVersion",
     "datasetId": 4192950
    },
    {
     "sourceId": 4298,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 3093
    }
   ],
   "dockerImageVersionId": 30559,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "INPUT_CTX_PATH = \"input/ctx\"\n",
    "FEATURE_PATH = \"input/feature\"\n",
    "OUTPUT_PATH = \"output\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T05:02:16.402990Z",
     "start_time": "2024-03-12T05:02:16.249889Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from time import time\n",
    "# import chromadbsl\n",
    "#from chromadb.config import Settings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "import os\n",
    "import openai\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-12-19T15:13:14.528082Z",
     "iopub.execute_input": "2023-12-19T15:13:14.528957Z",
     "iopub.status.idle": "2023-12-19T15:13:21.393724Z",
     "shell.execute_reply.started": "2023-12-19T15:13:14.528919Z",
     "shell.execute_reply": "2023-12-19T15:13:21.392929Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-12T05:02:16.814774Z",
     "start_time": "2024-03-12T05:02:16.789084Z"
    }
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialize model, tokenizer, query pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the model, the device, and the `bitsandbytes` configuration."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# !pip install python-docx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T05:02:22.329053Z",
     "start_time": "2024-03-12T05:02:22.289517Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import docx\n",
    "import base64\n",
    "\n",
    "\n",
    "def handledocx(filepath):\n",
    "    doc = docx.Document(filepath)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    resp = {\n",
    "      \"type\": \"text\",\n",
    "      \"text\": '\\n'.join(fullText)\n",
    "    }\n",
    "    return resp\n",
    "\n",
    "\n",
    "def handlepng(filepath):\n",
    "\n",
    "    with open(filepath, \"rb\") as image_file:\n",
    "         base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    resp =   {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "          }\n",
    "    }\n",
    "    return resp\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T05:02:24.137040Z",
     "start_time": "2024-03-12T05:02:24.122976Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsupported file type, RoadTrip.pdf, 'pdf'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# print(os.listdir(INPUT_PATH))\n",
    "supported_files = {\n",
    "    \"png\": handlepng,\n",
    "    # \"pdf\": \"handlepdf\",\n",
    "    \"docx\": handledocx,\n",
    "    # \"txt\": \"handle txt\",\n",
    "}\n",
    "\n",
    "\n",
    "def getcontent(path):\n",
    "    content = []\n",
    "    input_files = [f for f in os.listdir(path) if os.path.isfile(f'{path}/{f}')]\n",
    "    \n",
    "    for i in input_files:\n",
    "    \n",
    "        ext = i.split(\".\")[-1]\n",
    "        try:\n",
    "            inp = supported_files[ext](f'{path}/{i}')\n",
    "            content.append(inp)\n",
    "        except Exception as e:\n",
    "            print(f\"unsupported file type, {i}, {e}\")\n",
    "    return content\n",
    "\n",
    "def promptobj(prompt):\n",
    "    return {\n",
    "      \"type\": \"text\",\n",
    "      \"text\": prompt\n",
    "    }\n",
    "\n",
    "pre_prompt = promptobj(\"You are a product tester who will write test cases for the following project.\")\n",
    "\n",
    "ctx_content = getcontent(INPUT_CTX_PATH)\n",
    "\n",
    "mid_prompt = promptobj(\"For the above product we are building a new feature with the below description.\")\n",
    "\n",
    "feature_content = getcontent(FEATURE_PATH)\n",
    "\n",
    "end_prompt = promptobj(\"Generate 50 test cases for this new feature of the format, (test_case_name, test_case_description) for this products. Do not say anything else. Only provide csv compatible text output. and do not generate less than 50 test cases\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T05:03:21.365915Z",
     "start_time": "2024-03-12T05:03:21.165556Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T05:03:22.483046Z",
     "start_time": "2024-03-12T05:03:22.448336Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-91ob6LFk2v7w27cgzCOxPAkJ0X1KV', 'object': 'chat.completion', 'created': 1710219852, 'model': 'gpt-4-1106-vision-preview', 'usage': {'prompt_tokens': 3315, 'completion_tokens': 1142, 'total_tokens': 4457}, 'choices': [{'message': {'role': 'assistant', 'content': '```\\ntest_case_name, test_case_description\\ntest_case_001, Verify that the homepage loads correctly without any broken elements or missing images\\ntest_case_002, Ensure that the \\'Meal Preferences\\' widget is visible and functional on the map view\\ntest_case_003, Check that the \\'Sign Up\\' page can be accessed and contains options to register with email, Facebook, and Google+\\ntest_case_004, Confirm that the search functionality returns appropriate results given a starting point and destination\\ntest_case_005, Test the responsiveness of the map when different filters under \\'Meal Preferences\\' are applied\\ntest_case_006, Validate the slider interaction for \\'Price range\\' under \\'Meal Preferences\\' changes the results accordingly\\ntest_case_007, Check that the \\'Distance to travel from Location\\' slider correctly adjusts the search radius on the map\\ntest_case_008, Ensure that the review filter correctly filters the establishments based on the selected star rating\\ntest_case_009, Verify that the map updates in real time with the applied filters without crashing or stalling\\ntest_case_010, Ensure that clicking a marker on the map opens a popup with detailed information about the establishment\\ntest_case_011, Confirm that the \\'Save Trip by Login in\\' feature prompts the user to log in with proper authentication flow\\ntest_case_012, Test the ability to send directions to the user\\'s phone from the trip summary card\\ntest_case_013, Validate that the trip details provide accurate and comprehensive information (time, distance, route)\\ntest_case_014, Ensure that the \\'Meal Preferences\\' selections can be reset to default with a reset or clear button\\ntest_case_015, Test that the correct number of adult passengers can be selected in the \\'Profile\\' section\\ntest_case_016, Validate the upper and lower limit for the number of seniors (65+) in the \\'Profile\\' section\\ntest_case_017, Check that the youth (12-17) counter increments and decrements correctly and has appropriate limits\\ntest_case_018, Confirm that adding a \\'Child\\' (2-11) updates the total headcount and is limited correctly\\ntest_case_019, Verify that the infant seat selection is functioning and enforce a reasonable limit\\ntest_case_020, Check for proper error messages when trying to input invalid data in the \\'Profile\\' section\\ntest_case_021, Ensure that the user can switch between \\'Map\\' and \\'Satellite\\' view seamlessly\\ntest_case_022, Test that the maps zoom in/out functionality works without glitches or delays\\ntest_case_023, Confirm that the location search feature within the map provides accurate results\\ntest_case_024, Validate the robustness of the authentication flow when a user attempts to save a trip\\ntest_case_025, Check that the meal type dropdown contains all the options and that each option returns the correct results\\ntest_case_026, Test if the application retains the applied filters on the map when a user navigates away and returns to the map\\ntest_case_027, Confirm that the price range slider has proper minimum and maximum limits\\ntest_case_028, Verify that the user can set a specific distance for the \\'Distance to travel from Location\\' feature\\ntest_case_029, Ensure that filtering by \\'Reviews\\' returns only those locations with the selected number of stars or higher\\ntest_case_030, Check that the application correctly calculates and displays the route on the map after a search\\ntest_case_031, Confirm that the user can create a profile without errors or hiccups\\ntest_case_032, Ensure that the meal preferences can be customized with multiple meal types selected simultaneously\\ntest_case_033, Verify the functionality of the sorting feature, arranging establishments from nearest to furthest\\ntest_case_034, Check the successful sign-up process with proper validation for email format and password strength\\ntest_case_035, Validate the login system with correct and incorrect credentials to ensure security\\ntest_case_036, Test for cross-browser compatibility of the new feature on Chrome, Firefox, Safari, and Edge\\ntest_case_037, Ensure the application\\'s UI adjusts correctly when viewed on mobile devices and tablets (responsiveness)\\ntest_case_038, Confirm that the user receives a confirmation email after signing up\\ntest_case_039, Verify the password reset functionality when a user forgets their password\\ntest_case_040, Test that the user\\'s selected preferences are saved in their profile after logging out and back in\\ntest_case_041, Confirm that the map markers correspond with the right establishments in the list view (if applicable)\\ntest_case_042, Validate that users can share their trip details via social media or email with the correct permissions\\ntest_case_043, Check that loading times for map search results are within acceptable timeframes\\ntest_case_044, Verify that the terms of use and privacy policy are accessible and up to date\\ntest_case_045, Ensure that all interactive elements on the map (e.g., zoom buttons, view switch) have proper accessibility labels\\ntest_case_046, Confirm that you can navigate through the \\'Meal Preferences\\' options using keyboard shortcuts for accessibility\\ntest_case_047, Validate that the autocomplete feature for address input fields provides correct suggestions\\ntest_case_048, Test the feature to \"Save Trip\" for logged-in users, ensuring the trip gets added to their account without issues\\ntest_case_049, Ensure that there are no duplicate entries of restaurants/establishments for the same filters applied\\ntest_case_050, Validate the performance and stability of the map when zoomed in to the maximum level and fully interacted with\\n```'}, 'finish_reason': 'stop', 'index': 0}]}\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "# OpenAI API Key\n",
    "with open(\"secrets.txt\",\"r\") as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"path_to_your_image.jpg\"\n",
    "\n",
    "\n",
    "\n",
    "headers = {\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "payload = {\n",
    "  \"model\": \"gpt-4-vision-preview\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [pre_prompt,*ctx_content, mid_prompt, *feature_content, end_prompt]\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 3000\n",
    "}\n",
    "\n",
    "# print(payload)\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "print(response.json())\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T05:05:47.560301Z",
     "start_time": "2024-03-12T05:04:06.977414Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "with open(f'{OUTPUT_PATH}/output.csv', \"w+\") as f:\n",
    "    f.write(response.json()['choices'][0]['message']['content'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T05:06:05.837845Z",
     "start_time": "2024-03-12T05:06:05.819997Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = '<OPENAI API KEY>'\n",
    "# \n",
    "# EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "# \n",
    "# embedding_function = OpenAIEmbeddingFunction(api_key=os.environ.get('OPENAI_API_KEY'), model_name=EMBEDDING_MODEL)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:26:48.578809Z",
     "start_time": "2024-03-11T18:26:48.562998Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !ls /kaggle/input/llama-2/pytorch/7b-chat-hf/1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-19T16:03:52.005295Z",
     "iopub.execute_input": "2024-01-19T16:03:52.005646Z",
     "iopub.status.idle": "2024-01-19T16:03:52.950063Z",
     "shell.execute_reply.started": "2024-01-19T16:03:52.005617Z",
     "shell.execute_reply": "2024-01-19T16:03:52.949094Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:18:51.277939Z",
     "start_time": "2024-03-11T18:18:51.262084Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# model_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n",
    "# \n",
    "# device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "# \n",
    "# # set quantization configuration to load large model with less GPU memory\n",
    "# # this requires the `bitsandbytes` library\n",
    "# bnb_config = transformers.BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type='nf4',\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=bfloat16\n",
    "# )"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-12-19T15:13:23.277152Z",
     "iopub.execute_input": "2023-12-19T15:13:23.277518Z",
     "iopub.status.idle": "2023-12-19T15:13:23.319915Z",
     "shell.execute_reply.started": "2023-12-19T15:13:23.277486Z",
     "shell.execute_reply": "2023-12-19T15:13:23.319126Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare the model and the tokenizer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# time_1 = time()\n",
    "# model_config = transformers.AutoConfig.from_pretrained(\n",
    "#     model_id,\n",
    "# )\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     trust_remote_code=True,\n",
    "#     config=model_config,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map='auto',\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# time_2 = time()\n",
    "# print(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-12-19T15:15:17.457132Z",
     "iopub.execute_input": "2023-12-19T15:15:17.457505Z",
     "iopub.status.idle": "2023-12-19T15:17:16.375404Z",
     "shell.execute_reply.started": "2023-12-19T15:15:17.457475Z",
     "shell.execute_reply": "2023-12-19T15:17:16.374484Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf42488b15f44230bd886122a616725e"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Prepare model, tokenizer: 118.913 sec.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the query pipeline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "time_1 = time()\n",
    "query_pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",)\n",
    "time_2 = time()\n",
    "print(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-12-19T15:18:45.373228Z",
     "iopub.execute_input": "2023-12-19T15:18:45.373620Z",
     "iopub.status.idle": "2023-12-19T15:18:47.081840Z",
     "shell.execute_reply.started": "2023-12-19T15:18:45.373590Z",
     "shell.execute_reply": "2023-12-19T15:18:47.080919Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": "Prepare pipeline: 1.703 sec.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define a function for testing the pipeline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def get_output(tokenizer, pipeline, prompt_to_test):\n",
    "    \"\"\"\n",
    "    Perform a query\n",
    "    print the result\n",
    "    Args:\n",
    "        tokenizer: the tokenizer\n",
    "        pipeline: the pipeline\n",
    "        prompt_to_test: the prompt\n",
    "    Returns\n",
    "        None\n",
    "    \"\"\"\n",
    "    # adapted from https://huggingface.co/blog/llama2#using-transformers\n",
    "    time_1 = time()\n",
    "    sequences = pipeline(\n",
    "        prompt_to_test,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,)\n",
    "    time_2 = time()\n",
    "    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-12-19T15:18:52.118540Z",
     "iopub.execute_input": "2023-12-19T15:18:52.118918Z",
     "iopub.status.idle": "2023-12-19T15:18:52.125132Z",
     "shell.execute_reply.started": "2023-12-19T15:18:52.118888Z",
     "shell.execute_reply": "2023-12-19T15:18:52.124067Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the query pipeline\n",
    "\n",
    "We test the pipeline with a query about the meaning of State of the Union (SOTU)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "get_output(tokenizer,\n",
    "           query_pipeline,\n",
    "           \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-12-19T15:19:03.871565Z",
     "iopub.execute_input": "2023-12-19T15:19:03.871935Z",
     "iopub.status.idle": "2023-12-19T15:19:11.463843Z",
     "shell.execute_reply.started": "2023-12-19T15:19:03.871906Z",
     "shell.execute_reply": "2023-12-19T15:19:11.462919Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Test inference: 7.588 sec.\nResult: Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\nThe State of the Union address is an annual speech given by the President of the United States to a joint session of Congress, typically held in February or March, in which the President reviews the current state of affairs in the country and presents legislative proposals and policy initiatives for the upcoming year.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import csv\n",
    "from itertools import islice\n",
    "\n",
    "cfp = \"/kaggle/input/amazon-test/amcm.csv\"\n",
    "\n",
    "import csv\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "with open(cfp, encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in islice(reader,1,3):\n",
    "        template = \"\"\"Task: I will give you a product review from Amazon and you will have to tell me if that string contains potentially medically sensitive information and help me rephrase it if so.\n",
    "\n",
    "            Input: %s\n",
    "\n",
    "            Output: A JSON String containing 3 keys, nothing else.\n",
    "            \"sensitive\" as boolean value indicating whether it contains potentially medically sensitive information.\n",
    "\n",
    "            \"rephrasing\" as a string which contains the original string to be without privacy infringing information, don't modify unrelated words. If not sensitive, keep it blank.\n",
    "\n",
    "            \"reason\" as a string with exact copy of the sensitive part from the original sentence and brief explanation. If not sensitive, keep it blank.\n",
    "            \"\"\"\n",
    "#         print(row[3])\n",
    "        print(get_output(tokenizer, query_pipeline, template.format(row[3])))\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-19T15:26:30.977163Z",
     "iopub.execute_input": "2023-12-19T15:26:30.977573Z",
     "iopub.status.idle": "2023-12-19T15:26:39.085827Z",
     "shell.execute_reply.started": "2023-12-19T15:26:30.977537Z",
     "shell.execute_reply": "2023-12-19T15:26:39.084922Z"
    },
    "trusted": true
   },
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "text": "Test inference: 4.065 sec.\nResult: Task: I will give you a product review from Amazon and you will have to tell me if that string contains potentially medically sensitive information and help me rephrase it if so.\n\n            Input: %s\n\n            Output: A JSON String containing 3 keys, nothing else.\n            \"sensitive\" as boolean value indicating whether it contains potentially medically sensitive information.\n\n            \"rephrasing\" as a string which contains the original string to be without privacy infringing information, don't modify unrelated words. If not sensitive, keep it blank.\n\n            \"reason\" as a string with exact copy of the sensitive part from the original sentence and brief explanation. If not sensitive, keep it blank.\n            \nExample:\n{\n\"sensitive\": true,\n\"rephrasing\": \"Find out how this product can help you achieve the best results with minimal effort.\",\n\"reason\": \"This sentence\nNone\nTest inference: 4.035 sec.\nResult: Task: I will give you a product review from Amazon and you will have to tell me if that string contains potentially medically sensitive information and help me rephrase it if so.\n\n            Input: %s\n\n            Output: A JSON String containing 3 keys, nothing else.\n            \"sensitive\" as boolean value indicating whether it contains potentially medically sensitive information.\n\n            \"rephrasing\" as a string which contains the original string to be without privacy infringing information, don't modify unrelated words. If not sensitive, keep it blank.\n\n            \"reason\" as a string with exact copy of the sensitive part from the original sentence and brief explanation. If not sensitive, keep it blank.\n            \n    Sample Output:\n    {\n        \"sensitive\": true,\n        \"rephrasing\": \"This product has a great price!\",\n        \"reason\": \"Price\"\n   \nNone\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Retrieval Augmented Generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check the model with a HuggingFace pipeline\n",
    "\n",
    "\n",
    "We check the model with a HF pipeline, using a query about the meaning of State of the Union (SOTU)."
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-23T19:22:16.433666Z",
     "iopub.execute_input": "2023-09-23T19:22:16.434937Z",
     "iopub.status.idle": "2023-09-23T19:22:16.440864Z",
     "shell.execute_reply.started": "2023-09-23T19:22:16.434891Z",
     "shell.execute_reply": "2023-09-23T19:22:16.439217Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
    "# checking again that everything is working fine\n",
    "llm(prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-12-18T21:24:27.078303Z",
     "iopub.execute_input": "2023-12-18T21:24:27.078658Z",
     "iopub.status.idle": "2023-12-18T21:24:31.773465Z",
     "shell.execute_reply.started": "2023-12-18T21:24:27.078630Z",
     "shell.execute_reply": "2023-12-18T21:24:31.772519Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "execution_count": 15,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nThe State of the Union address is an annual speech given by the President of the United States to a joint session of Congress, in which the President reports on the current state of the union and outlines their legislative agenda for the upcoming year.'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ingestion of data using Text loder\n",
    "\n",
    "We will ingest the newest presidential address, from Jan 2023."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "loader = TextLoader(\"/kaggle/input/president-bidens-state-of-the-union-2023/biden-sotu-2023-planned-official.txt\",\n",
    "                    encoding=\"utf8\")\n",
    "documents = loader.load()"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-12-18T21:22:23.721065Z",
     "iopub.execute_input": "2023-12-18T21:22:23.721947Z",
     "iopub.status.idle": "2023-12-18T21:22:23.737413Z",
     "shell.execute_reply.started": "2023-12-18T21:22:23.721912Z",
     "shell.execute_reply": "2023-12-18T21:22:23.736660Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split data in chunks\n",
    "\n",
    "We split data in chunks using a recursive character text splitter."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(documents)"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-12-18T21:22:27.586623Z",
     "iopub.execute_input": "2023-12-18T21:22:27.586981Z",
     "iopub.status.idle": "2023-12-18T21:22:27.613175Z",
     "shell.execute_reply.started": "2023-12-18T21:22:27.586954Z",
     "shell.execute_reply": "2023-12-18T21:22:27.612247Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Embeddings and Storing in Vector Store"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the embeddings using Sentence Transformer and HuggingFace embeddings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-12-18T21:22:41.534799Z",
     "iopub.execute_input": "2023-12-18T21:22:41.535652Z",
     "iopub.status.idle": "2023-12-18T21:22:48.656641Z",
     "shell.execute_reply.started": "2023-12-18T21:22:41.535619Z",
     "shell.execute_reply": "2023-12-18T21:22:48.655807Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading .gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9a4f77255c744729b3816dfa0e9b7b8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c1a1573c345434b8d7b7c64ec3ac9c2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4161dae0829f4dcab2c69ab99471dfc5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7ad1af72c264fb2b84364af7b74a4b8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c925e0e3e2442338e15449b09a06ee0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29474b75c69f420a9df185bb80734fd5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cada03478e52418aa96727ea828e49a3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "12408efed09c4a368b595b555bc4e8a3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00099cc0b7ad4578a425ac93830ab4a4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "015d3a7563174655aa72af59f94bf2c6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "146062a578074effb898462e3a3f6ccd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a466c644f4347e6ad6c64e14b36dc4e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8aab8f8e33c641bbab966a81e8a98af2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92988b19e2ee4dc093dc4eaf4b626e47"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize ChromaDB with the document splits, the embeddings defined previously and with the option to persist it locally."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-12-18T21:23:02.793431Z",
     "iopub.execute_input": "2023-12-18T21:23:02.793779Z",
     "iopub.status.idle": "2023-12-18T21:23:03.767052Z",
     "shell.execute_reply.started": "2023-12-18T21:23:02.793751Z",
     "shell.execute_reply": "2023-12-18T21:23:03.766262Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3ec82592a1b4f629cc8a9aa32fb81a6"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialize chain"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-12-18T21:24:39.287835Z",
     "iopub.execute_input": "2023-12-18T21:24:39.288200Z",
     "iopub.status.idle": "2023-12-18T21:24:39.293974Z",
     "shell.execute_reply.started": "2023-12-18T21:24:39.288162Z",
     "shell.execute_reply": "2023-12-18T21:24:39.292901Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the Retrieval-Augmented Generation \n",
    "\n",
    "\n",
    "We define a test function, that will run the query and time it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def test_rag(qa, query):\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    time_1 = time()\n",
    "    result = qa.run(query)\n",
    "    time_2 = time()\n",
    "    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n",
    "    print(\"\\nResult: \", result)"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-12-18T21:24:42.792919Z",
     "iopub.execute_input": "2023-12-18T21:24:42.793331Z",
     "iopub.status.idle": "2023-12-18T21:24:42.798734Z",
     "shell.execute_reply.started": "2023-12-18T21:24:42.793299Z",
     "shell.execute_reply": "2023-12-18T21:24:42.797772Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check few queries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\"\n",
    "test_rag(qa, query)"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-12-18T21:28:03.716646Z",
     "iopub.execute_input": "2023-12-18T21:28:03.717022Z",
     "iopub.status.idle": "2023-12-18T21:28:15.428052Z",
     "shell.execute_reply.started": "2023-12-18T21:28:03.716994Z",
     "shell.execute_reply": "2023-12-18T21:28:15.427107Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": "Query: What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\n\n\n\n\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf791b582a384b8e8d07bf6637c3a018"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\n\u001B[1m> Finished chain.\u001B[0m\nInference time: 11.707 sec.\n\nResult:   The State of the Union in 2023 focused on several key topics, including the nation's economic strength, the competition with China, and the need to come together as a nation to face the challenges ahead. The President emphasized the importance of American innovation, industries, and military modernization to ensure the country's safety and stability. The President also highlighted the nation's resilience and optimism, urging Americans to see each other as fellow citizens and to work together to overcome the challenges facing the country.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"What is the nation economic status? Summarize. Keep it under 200 words.\"\n",
    "test_rag(qa, query)"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-12-18T21:48:22.084995Z",
     "iopub.execute_input": "2023-12-18T21:48:22.085388Z",
     "iopub.status.idle": "2023-12-18T21:48:32.644864Z",
     "shell.execute_reply.started": "2023-12-18T21:48:22.085358Z",
     "shell.execute_reply": "2023-12-18T21:48:32.643921Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": "Query: What is the nation economic status? Summarize. Keep it under 200 words.\n\n\n\n\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "137021f18e624f27a3c5386aaaea3c57"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\n\u001B[1m> Finished chain.\u001B[0m\nInference time: 10.555 sec.\n\nResult:   The nation's economic status is strong, with a low unemployment rate of 3.4%, near record lows for Black and Hispanic workers, and fastest growth in 40 years in manufacturing jobs. The president highlights the progress made in creating good-paying jobs, exporting American products, and reducing inflation. However, the president acknowledges there is still more work to be done to fully recover from the pandemic and Putin's war.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Document sources\n",
    "\n",
    "Let's check the documents sources, for the last query run."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "docs = vectordb.similarity_search(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved documents: {len(docs)}\")\n",
    "for doc in docs:\n",
    "    doc_details = doc.to_json()['kwargs']\n",
    "    print(\"Source: \", doc_details['metadata']['source'])\n",
    "    print(\"Text: \", doc_details['page_content'], \"\\n\")"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-10-27T20:55:54.885741Z",
     "iopub.execute_input": "2023-10-27T20:55:54.88613Z",
     "iopub.status.idle": "2023-10-27T20:55:54.935969Z",
     "shell.execute_reply.started": "2023-10-27T20:55:54.886095Z",
     "shell.execute_reply": "2023-10-27T20:55:54.935116Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusions\n",
    "\n",
    "\n",
    "We used Langchain, ChromaDB and Llama 2 as a LLM to build a Retrieval Augmented Generation solution. For testing, we were using the latest State of the Union address from Jan 2023.\n",
    "\n",
    "\n",
    "# More work on the same topic\n",
    "\n",
    "You can find more details about how to use a LLM with Kaggle. Few interesting topics are treated in:  \n",
    "\n",
    "* https://www.kaggle.com/code/gpreda/test-llama-2-quantized-with-llama-cpp (quantizing LLama 2 model using llama.cpp)\n",
    "* https://www.kaggle.com/code/gpreda/fast-test-of-llama-v2-pre-quantized-with-llama-cpp  (quantized Llamam 2 model using llama.cpp)  \n",
    "* https://www.kaggle.com/code/gpreda/test-of-llama-2-quantized-with-llama-cpp-on-cpu (quantized model using llama.cpp - running on CPU)  \n",
    "* https://www.kaggle.com/code/gpreda/explore-enron-emails-with-langchain-and-llama-v2 (Explore Enron Emails with Langchain and Llama v2)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References  \n",
    "\n",
    "[1] Murtuza Kazmi, Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data, https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476  \n",
    "\n",
    "[2] Patrick Lewis, Ethan Perez, et. al., Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, https://browse.arxiv.org/pdf/2005.11401.pdf \n",
    "\n",
    "[3] Minhajul Hoque, Retrieval Augmented Generation: Grounding AI Responses in Factual Data, https://medium.com/@minh.hoque/retrieval-augmented-generation-grounding-ai-responses-in-factual-data-b7855c059322  \n",
    "\n",
    "[4] Fangrui Liu\t, Discover the Performance Gain with Retrieval Augmented Generation, https://thenewstack.io/discover-the-performance-gain-with-retrieval-augmented-generation/\n",
    "\n",
    "[5] Andrew, How to use Retrieval-Augmented Generation (RAG) with Llama 2, https://agi-sphere.com/retrieval-augmented-generation-llama2/   \n",
    "\n",
    "[6] Yogendra Sisodia, Retrieval Augmented Generation Using Llama2 And Falcon, https://medium.com/@scholarly360/retrieval-augmented-generation-using-llama2-and-falcon-ed26c7b14670   \n",
    "\n"
   ],
   "metadata": {}
  }
 ]
}
